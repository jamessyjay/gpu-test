# syntax=docker/dockerfile:1.7-labs
###############################################
# Build from scratch with Ubuntu + micromamba #
###############################################

# ---- GPU target (CUDA runtime + micromamba + PyTorch CUDA) ----
FROM nvidia/cuda:12.4.0-runtime-ubuntu22.04 AS gpu

ENV DEBIAN_FRONTEND=noninteractive     MAMBA_ROOT_PREFIX=/opt/conda     PIP_NO_CACHE_DIR=1     PYTHONDONTWRITEBYTECODE=1     PYTHONUNBUFFERED=1     NVIDIA_VISIBLE_DEVICES=all     NVIDIA_DRIVER_CAPABILITIES=compute,utility     NCCL_DEBUG=INFO

RUN --mount=type=cache,target=/var/cache/apt \
    apt-get update && apt-get install -y --no-install-recommends \
      ca-certificates curl bzip2 tini python3 python3-venv \
    && rm -rf /var/lib/apt/lists/*

# Install micromamba via tar.bz2 (auto-arch)
ARG MAMBA_VERSION=1.5.10
ARG TARGETARCH
RUN set -eux; \
    case "${TARGETARCH}" in \
      amd64) MM_ARCH=linux-64 ;; \
      arm64) MM_ARCH=linux-aarch64 ;; \
      *) echo "Unsupported TARGETARCH=${TARGETARCH}" >&2; exit 1 ;; \
    esac; \
    curl -fsSL "https://micro.mamba.pm/api/micromamba/${MM_ARCH}/${MAMBA_VERSION}" -o /tmp/micromamba.tar.bz2; \
    tar -xvjf /tmp/micromamba.tar.bz2 -C /usr/local/bin --strip-components=1 bin/micromamba; \
    chmod +x /usr/local/bin/micromamba; rm -f /tmp/micromamba.tar.bz2

# Create env from spec (GPU)
COPY conda-env-gpu.yml /tmp/env.yml
SHELL ["/bin/bash", "-lc"]
RUN micromamba create -y -n app -f /tmp/env.yml \
 && micromamba clean -a -y
ENV PATH=${MAMBA_ROOT_PREFIX}/envs/app/bin:$PATH

# App
WORKDIR /app
COPY requirements.txt ./requirements.txt
RUN python -m pip install --no-cache-dir -r requirements.txt
# Install PyTorch with CUDA 12.4 from official wheels
RUN python -m pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cu124 \
    torch==2.5.* torchvision==0.20.* torchaudio==2.5.*
COPY src/ ./src/
ENV PYTHONPATH=/app/src

ENTRYPOINT ["/usr/bin/tini", "--"]
CMD ["python","src/run_all_tests.py","--quick"]
